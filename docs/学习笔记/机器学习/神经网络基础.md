# 神经网络基础

## 1. 神经网络概述

神经网络是一种模仿生物神经网络结构和功能的计算模型，由大量的神经元相互连接而成。

### 1.1 神经元模型

基本的神经元模型（感知机）由以下部分组成：

- 输入层：接收外部输入
- 权重：连接强度
- 偏置：阈值调整
- 激活函数：输出非线性转换

```math
y = f(w_1x_1 + w_2x_2 + ... + w_nx_n + b)
```

### 1.2 激活函数

常用的激活函数：

1. **Sigmoid**：将输入映射到(0,1)区间
   ```math
sigmoid(x) = \frac{1}{1 + e^{-x}}
   ```

2. **Tanh**：将输入映射到(-1,1)区间
   ```math
tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
   ```

3. **ReLU**：修正线性单元
   ```math
ReLU(x) = max(0, x)
   ```

4. **Leaky ReLU**：带泄漏的ReLU
   ```math
LeakyReLU(x) = max(0.01x, x)
   ```

## 2. 神经网络结构

### 2.1 前馈神经网络

前馈神经网络是最基本的神经网络结构，信息从输入层流向输出层，没有反馈连接。

- **输入层**：接收原始数据
- **隐藏层**：提取特征表示
- **输出层**：产生最终结果

### 2.2 深度神经网络

深度神经网络（DNN）是指具有多个隐藏层的神经网络。

- 浅层网络：1-2个隐藏层
- 深层网络：3个以上隐藏层

## 3. 反向传播算法

反向传播算法是训练神经网络的核心算法，通过链式法则计算梯度，然后使用梯度下降更新权重。

### 3.1 梯度下降

梯度下降是一种优化算法，用于最小化损失函数。

```math
w = w - \alpha \nabla_w L(w)
```

其中：
- \(\alpha\) 是学习率
- \(\nabla_w L(w)\) 是损失函数对权重的梯度

### 3.2 反向传播步骤

1. 前向传播：计算网络输出
2. 计算损失：比较输出与真实值
3. 反向传播：计算梯度
4. 更新权重：使用梯度下降

## 4. 常见神经网络类型

### 4.1 卷积神经网络（CNN）

- 主要用于图像识别
- 包含卷积层、池化层和全连接层
- 擅长提取空间特征

### 4.2 循环神经网络（RNN）

- 主要用于序列数据
- 包含循环连接，能够处理时序信息
- 擅长处理文本、语音等序列数据

### 4.3 长短期记忆网络（LSTM）

- RNN的改进版本
- 解决了长期依赖问题
- 包含遗忘门、输入门和输出门

### 4.4 生成对抗网络（GAN）

- 由生成器和判别器组成
- 生成器生成假数据，判别器区分真假
- 用于生成图像、文本等

## 5. 训练技巧

### 5.1 正则化

- **L1正则化**：添加权重绝对值之和
- **L2正则化**：添加权重平方和
- **Dropout**：随机丢弃神经元
- **Batch Normalization**：批归一化

### 5.2 优化算法

- **SGD**：随机梯度下降
- **Momentum**：动量法
- **RMSProp**：均方根传播
- **Adam**：自适应矩估计

### 5.3 学习率调度

- 固定学习率
- 学习率衰减
- 余弦退火
- 自适应学习率

## 6. 应用领域

- 计算机视觉：图像分类、目标检测、图像生成
- 自然语言处理：机器翻译、情感分析、文本生成
- 语音识别：语音转文字、语音合成
- 推荐系统：商品推荐、内容推荐
- 强化学习：游戏AI、机器人控制

## 7. 挑战与未来

### 7.1 当前挑战

- 计算资源需求大
- 可解释性差
- 数据依赖强
- 容易过拟合

### 7.2 未来发展方向

- 深度学习与符号AI结合
- 小样本学习
- 自监督学习
- 神经架构搜索
- 可解释AI

## 8. 实践建议

1. 从简单模型开始，逐步增加复杂度
2. 合理划分训练集、验证集和测试集
3. 使用合适的评估指标
4. 可视化训练过程
5. 尝试不同的超参数组合
6. 注意防止过拟合

## 9. 常用框架

- TensorFlow
- PyTorch
- Keras
- MXNet
- Caffe

## 10. 学习资源

- 《深度学习》（花书）
- 《动手学深度学习》
- Coursera深度学习专项课程
- fast.ai课程
- 各种开源项目和教程